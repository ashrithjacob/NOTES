Summary:
In batch normalization, the mean and variance are calculated for each batch(mew and sigma) and the data is normalised by these values (x' = (x-mew)/sigma). Here the mew and sigma are from the mean and standard deviation of the batch (all done in model.train() mode).
the data is then scaled by gamma and shifted by beta (y = gamma*x' + beta). This is done in both model.train() and model.eval() modes. Here gamma and beta are learnable parameters of the model.
In testing we use the statistics calculated during training. This is why we need to call model.eval() before testing. If you donâ€™t call model.eval() before testing, the model will use the batch statistics calculated during training, which will not be accurate. This is why you get different results when you call model.eval() before testing.
So how is batch normalisation calculated for the test set?
During training a running mean and std are calculated as follows:
```
running_mean = momentum * running_mean + (1 - momentum) * sample_mean
running_var = momentum * running_var + (1 - momentum) * sample_var
```
During testing, this running mean and std are used to normalise the data (not the batch mean and std). This is why you need to call model.eval() before testing.

Intuition:
We can think of the batch norm layer as a linear transformation of the input data. The linear transformation is parameterized by two learnable parameters, gamma and beta. The linear transformation is applied to the input data in both training and testing. The difference is that in training, the mean and std of the input data are calculated for each batch and used to normalize the data. In testing, the mean and std of the input data are calculated for the entire training set and used to normalize the data. This is why we need to call model.eval() before testing.
Batch norm thus in a sense tries to decouple the input data to a layer from the layers before by simply applying the 'best possible' linear transformation to the input data. This is why batch norm is used in the middle of a network and not at the beginning or end.


